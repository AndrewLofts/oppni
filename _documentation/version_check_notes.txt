

====================================================================================
 VERSION_CHECK_NOTES: this file provides notes summarizing how to test consistency
 between outputs of different pipeline versions using "OPPNI_version_check.m" 
====================================================================================

.This file is used to compare the full outputs generated by different pipeline versions,
 in order to (1) identify inconsistencies that are accidentally introduced in the results,
 or (2) determine the overall impact of changes in pipeline algorithms.

.This file must be run independently, after full pipeline results are generated using two
 different OPPNI packages, with respective input files "InputFile1", "InputFile2".
 NB: they should run the *exact same* pipelines and task designs!


% Syntax: 
%             out = OPPNI_version_check( InputFile1, InputFile2, out_name, distatis_flag );
%
% Input:       
%             InputFile1, InputFile2: strings, giving path+name of the input text-files
%                                     used to generate OPPNI outputs being compared
%             out_name              : name of output textfile summarizing comparison 
%                                     between OPPNI outputs
%             distatis_flag         : (optional) argument specifying if DISTATIS should be run
%                                     to compare optimized SPMs.
%
% Output:     textfile "out_name", which summarizes comparisons between
%             the two OPPNI results

====================================================================================

.The script performs 3 levels of comparison. They are as follows:

(1) Consistency Checks: this step performs basic checking, to determine if the results can be
    compared between files. This includes:

    > NUMBER OF DATA RUNS: checks that InputFile1 and InputFile2 have the same number of lines
        [ counts the #lines in InputFile1, InputFile2 ]
    > NUMBER OF CONTRAST SPMs: checks that the same number of task contrasts are analyzed
      for each subject (in case of multi-contrast analysis)
        [ counts #columns in the (voxel x contrast) SPM matrix of SPM_opt{1}.con, in "optimization_summary.mat" ]
    > ALL METRICS CONSISTENT: checks whether both OPPNI versions generate the same set of NPAIRS
      metrics for each pipeline (e.g., prediction (P), reproducibility (R), dPR, etc.);
        [ matches field names of METRIC_opt.con, in "optimization_summary.mat" ]
        [ terminates if no consistent metrics; otherwise warns the user, lists inconsistent metrics ]
    > SAME NUMBER OF BRAIN VOXELS: checks that masked SPMs have same number of voxels for each subject
        [ counts #rows in the (voxel x contrast) SPM matrix of SPM_opt{i}.con (i=1:Nsubject), in "optimization_summary.mat" ]
        [ skips any subsequent comparisons of SPMs if not consistent, lists the number of inconsistent subjects ]
    > NUMBER OF PIPELINE STEPS CONSISTENT: checks list of available pipeline steps
        [ compares cell array of names of pipeline steps, pipeline_sets.pipenames stored in "optimization_summary.mat" ]
        [ lists any steps that are not present in both, unmatched pipelines excluded from further analysis ]
    > SAME OPTIMIZATION METRIC: checks that pipelines are optimized using the same criterion (e.g. 'dPR')
        [ compares string denoting optimization metric pipeline_sets.optimize_metric, in "optimization_summary.mat" ]
        [ warns user if they don't match, but continues with checks ]

(2) Optimization Outputs: if initial checks passed, this step now compares the optimization SPMs
    and associated metrics. This includes;

    > COMPARING OPTIMAL PIPELINES: checks if OPPNI versions have selected the same optimal pipeline set for every subject
        [ compares pipeline_sets.con, pipeline_sets.fix, pipeline_sets.ind, in "optimization_summary.mat" ]
        [ warns if inconsistent CON, FIX or IND (counts the number of runs w/ inconsistent IND pipelines) ]
    > COMPARING OPTIMIZED METRICS: for all metrics generated by both OPPNI versions, checks if consistent for CON, FIX, IND pipelines
        [ compares all elements of METRIC_opt, in "optimization_summary.mat" ]
        [ counts the number of runs with inconsistent metrics; measures mean abs. difference in metrics, for runs that are changed ]
    > COMPARING OPTIMIZED SPMs: checks if all voxel values are consistent for optimal SPMs of CON, FIX, IND pipelines
        [ compares all elements of SPM_opt cell array, in "optimization_summary.mat" ]
        [ counts the number of runs with inconsistent SPMs; measures mean correlation between SPMs, for runs that are changed ]

 (3) Intermediate Outputs: more extensive check of all intermediate pipeline results. If optimization outputs are not 
     consistent, this can help determine the extent of pipeline changes. This  includes:

    > SAME SET OF TESTED STEPS: confirms that optimization was based on the same total number of pipeline steps,
      as a sanity check (in case consistency checks miss something)
        [ compares size of pipeset structure, in "res3_stats/stats_<output>.mat" ]
        [ terminates here if inconsistent ]
    > COMPARING ALL PIPELINE METRICS: compares metrics of ALL pipelines that have been run.
        [ compares elements of METRIC_set, in "res3_stats/stats_<output>.mat" ]
        [ warns of inconsistencies, then:                                                                                        ]
        [   (1) counts the number of inconsistent runs                                                                           ] 
        [   (2) lists pipeline steps that alter output, OFF always gives same results, ON always gives different (or vice-versa) ]
    > COMPARING ALL PIPELINE SPMs: compares voxel values of ALL pipeline SPMs that have been generated
        [ compares elements of IMAGE_set, in "res3_stats/stats_<output>.mat" ]
        [ warns of inconsistencies, then:                                                                                        ]
        [   (1) counts the number of inconsistent runs                                                                           ] 
        [   (2) lists pipeline steps that alter output, OFF always gives same results, ON always gives different (or vice-versa) ]


.If inconsistent results are detected, and distatis_flag=1, script also runs DISTATIS analysis.
 This is a bootstrapped 3-way analysis technique which computes 95% CIs on the similarity between 
 pipeline SPMs. Produces a 6x6 supplemental table in outputs detailing mean bootstrapped correlations 
 between CON, FIX, IND pipeline outputs of the different code versions.


